{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import nnetwork as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist data set can, e.g., be found on this webpage:\n",
    "http://deeplearning.net/tutorial/gettingstarted.html\n",
    "Here, we just store the datafile in the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = './datasets/mnist.pkl.gz'\n",
    "with gzip.open(fp, 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the target values into one hot encoding.\n",
    "For this we define a small helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(targets):\n",
    "    n_examples = targets.shape[0]\n",
    "    one_hot_targets = np.zeros((n_examples, 10))\n",
    "    idx = np.arange(n_examples)\n",
    "    one_hot_targets[idx, targets] = 1.\n",
    "    return one_hot_targets\n",
    "\n",
    "target_train = one_hot_encoding(train_set[1])\n",
    "target_valid = one_hot_encoding(valid_set[1])\n",
    "test_valid = one_hot_encoding(test_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement a very basic loop for hyper parameter tuning of the l2-regularisation.\n",
    "In each loop occurence, we create a neural network with a different l2 value and train until our train-criterion is broken. We stop training if the loss after one epoch is only marginally lower than the previous epoch's loss by the value of the precision variable, which we set in the fit function of the neural network. We then evaluate the accuracy on the validation set and chose the best l2 parameter based on this metric. (We could also use the loss on the validation set)\n",
    "\n",
    "To initialise the network, we also have to define the number of layers and their number of nodes. Note, that only DenseLayers are implemented here thus far. We go for a very basic network to test our implementation. We connect the input layer directly with the output layer.\n",
    "\n",
    "The parameters of the network are initialised randomly from a Gaussian distribution with mean = 0. and sigma = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEURAL NETWORK created!\n",
      "2 layers in total (including In- and Output-layer)\n",
      "1 parameter matrices\n",
      "\n",
      "0 1.2 17.77255542234628\n",
      "10 1.2 2.355281528974366\n",
      "Final Cost: 2.1825213880962973\n",
      "final validation accuracy: 68.98\n",
      "\n",
      "NEURAL NETWORK created!\n",
      "2 layers in total (including In- and Output-layer)\n",
      "1 parameter matrices\n",
      "\n",
      "0 1.2 18.248673395708515\n",
      "10 0.6 2.4111839301868603\n",
      "20 0.6 1.9210821368936233\n",
      "30 0.6 1.7003980494574713\n",
      "40 0.6 1.5595286940342683\n",
      "50 0.6 1.4600579776799822\n",
      "60 0.6 1.385138351238439\n",
      "70 0.6 1.3261126613191232\n",
      "Final Cost: 1.30576873033237\n",
      "final validation accuracy: 83.25\n",
      "\n",
      "NEURAL NETWORK created!\n",
      "2 layers in total (including In- and Output-layer)\n",
      "1 parameter matrices\n",
      "\n",
      "0 1.2 25.650289376899973\n",
      "10 1.2 2.225695060282258\n",
      "Final Cost: 1.8139319853500517\n",
      "final validation accuracy: 75.68\n"
     ]
    }
   ],
   "source": [
    "n_layers = np.array([784, 10])\n",
    "l2s = [0.01, 0.1, 1.]\n",
    "best_acc = 0.\n",
    "best_net = None\n",
    "for l2 in l2s:\n",
    "    net = nn.NeuralNet(n_layers, l2)\n",
    "    net.train(train_set[0].T, target_train.T, precision=0.005)\n",
    "    y_pred = net.predict(valid_set[0].T)\n",
    "    acc = 100 * np.mean(np.where(y_pred==valid_set[1], 1, 0))\n",
    "    print 'final validation accuracy: {}'.format(acc)\n",
    "    if acc >= best_acc:\n",
    "        best_net = net\n",
    "        best_acc = acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the networks and found the best value of the l2 parameter, we would like to evaluate the final accuracies, for the train, validation and test set.\n",
    "For this, we simply use the predict function of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train accuracy: 81.912\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_net.predict(train_set[0].T)\n",
    "print 'final train accuracy: {}'.format(100 * np.mean(np.where(y_pred==train_set[1], 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation accuracy: 83.25\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_net.predict(valid_set[0].T)\n",
    "print 'final validation accuracy: {}'.format(100 * np.mean(np.where(y_pred==valid_set[1], 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test accuracy: 83.03\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_net.predict(test_set[0].T)\n",
    "print 'final test accuracy: {}'.format(100 * np.mean(np.where(y_pred==test_set[1], 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we can get a decent accuracy of hand-written digits recognition with our implementation of neural networks. This confirms that our implementation is correct (including the backpropagation).\n",
    "\n",
    "It is surprising, that simply by connecting the input to the output Layer, we can get an accuray on the test set of 83%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
